# -*- coding: utf-8 -*-
"""Heart disease analysis and prediction system (2).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nGuGd0i6is6pfER8eBEOE-89vyzRcA5u

# Heart disease analysis and prediction system

#### Importing Data form heart.csv
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
df=pd.read_csv('C:/Users/Dell/Desktop/HeartHealthPrediction-master/heart.csv')
df.head()

"""# Data Cleansing

#### Listing Null Values form data
"""

print(df.isnull().sum())

"""##### Droping Null values because Null values are very less as compared to data size"""

df=df.dropna()

"""##### Give the info of Data Type"""

df.info()

"""##### Describing the wohle Data"""

df.describe()

"""##### Pandas Profiling Provide the Report of data set including Corelation

#### Target Stats
"""

d=df['Target'].value_counts()
print(d)
No_Disease = len(df[df['Target'] ==0])
Diseased = len(df[df['Target'] ==1])

print('Percentage of No_Disease: {:.2f} %' .format(No_Disease/len(df['Target'])*100))
print('Percentage of Diseased: {:.2f} %' .format(Diseased/len(df['Target'])*100))

"""#### Age Stats"""

min_age = df['Age'].min()
max_age = df['Age'].max()
mean_age = round(df['Age'].mean(),1)

print('Min age: %s' %min_age)
print('Max age: %s' %max_age)
print('Mean age: %s' %mean_age)

"""#### Sex Stats"""

female = len(df[df['Sex'] ==0])
male = len(df[df['Sex'] ==1])

print('Percentage of female: {:.2f} %' .format(female/len(df['Sex'])*100))
print('Percentage of male: {:.2f} %' .format(male/len(df['Sex'])*100))

"""# Data Visualization"""

df.hist(bins=20,figsize=(15,15),grid=True,ec='black',color='violet')
plt.show()

plt.figure(figsize = (15,12))
sns.pairplot(df[['Age', 'RestBP', 'Chol', 'MaxHR', 'Oldpeak','Target']],hue='Target',palette='icefire')
plt.show()

"""##### Heart Diseases Ratio in Dataset
###### Blue Graph indicate no heart desease and Orange Graph show Heart desease
"""

def plotTarget():
    sns.countplot(x='Target', data=df, ax=ax)
    for i, p in enumerate(ax.patches):
        count=df['Target'].value_counts().values[i]
        x=p.get_x()+ p.get_width() /2.
        y=p.get_height() + 3
        label='{:1.2f}'.format(count / float(df.shape[0]))
        ax.text(x, y,label, ha='center')

fig_target,ax=plt.subplots(nrows=1, ncols=1, figsize=(5, 2))
plotTarget()

df.corr()

"""##### Select Age as most dependent data on label
###### Disease Probability Bar Plot
"""

def plotAge():
    facet_grid = sns.FacetGrid(df, hue='Target')
    facet_grid.map(sns.kdeplot, "Age", shade=True, ax=axes[0])
    legend_labels = ['Disease false', 'Disease true']
    for t, l in zip(axes[0].get_legend().texts, legend_labels):
        t.set_text(l)
        axes[0].set(xlabel='Age', ylabel='Density')

    avg = df[["Age", "Target"]].groupby(['Age'], as_index=False).mean()
    sns.barplot(x='Age', y='Target', data=avg, ax=axes[1])
    axes[1].set(xlabel='Age', ylabel='disease probability')

    plt.clf()

fig_age, axes = plt.subplots(nrows=2, ncols=1, figsize=(15, 8))
plotAge()

plt.figure(figsize=(12,10))
sns.heatmap(df.corr(),annot=True,cmap="magma",fmt='.2f')

"""##### Checking For Categorical Data"""

x=df['ChestPain']
x.value_counts()

x=df['Thal']
x.value_counts()

"""##### Ploting Function For Categorical Data " Chest Pain" && "Thalassemia"
##### Ploting Function For Continoius Data
"""

category=[('ChestPain', ['typical','nontypical','nonanginal','asymptomatic']),('Thal',['fixed','normal','reversable',])]
continuous = [('Age', 'Age in year'),
              ('Sex','1 for Male 0 for Female'),
              ('RestBP','BP in Rest State'),
              ('Fbs','Fasting blood glucose'),
              ('RestECG','ECG at rest'),
              ('Chol', 'serum cholestoral in mg/d'),
              ('MaxHR','Max Heart Rate'),
              ('ExAng','Exchange Rate'),
              ('Slope','Slope of Curve'),
              ('Oldpeak', 'ST depression by exercise relative to rest'),
              ('Ca', '# major vessels: (0-3) colored by flourosopy')]


def plotCategorial(attribute, labels, ax_index):
    sns.countplot(x=attribute, data=df, ax=axes[ax_index][0])
    sns.countplot(x='Target', hue=attribute, data=df, ax=axes[ax_index][1])
    avg = df[[attribute, 'Target']].groupby([attribute], as_index=False).mean()
    sns.barplot(x=attribute, y='Target', hue=attribute, data=avg, ax=axes[ax_index][2])

    for t, l in zip(axes[ax_index][1].get_legend().texts, labels):
        t.set_text(l)
    for t, l in zip(axes[ax_index][2].get_legend().texts, labels):
        t.set_text(l)


def plotContinuous(attribute, xlabel, ax_index):
    sns.distplot(df[[attribute]], ax=axes[ax_index][0])
    axes[ax_index][0].set(xlabel=xlabel, ylabel='density')
    sns.violinplot(x='Target', y=attribute, data=df, ax=axes[ax_index][1])


def plotGrid(isCategorial):
    if isCategorial:
        [plotCategorial(x[0], x[1], i) for i, x in enumerate(category)]
    else:
        [plotContinuous(x[0], x[1], i) for i, x in enumerate(continuous)]

"""##### Categorical Plot"""

fig_categorial,axes=plt.subplots(nrows=len(category), ncols=3, figsize=(10, 10))
plotGrid(isCategorial=True)

"""##### Continuous Plot"""

fig_continuous, axes = plt.subplots(nrows=len(continuous), ncols=2, figsize=(10,10))
plotGrid(isCategorial=False)

"""##### Creating Dummy"""

#dummy for chest Pain
chestpain_dummy = pd.get_dummies(df.loc[:,'ChestPain'])
chestpain_dummy.rename(columns={1: 'Typical', 2: 'Asymptomatic',3: 'Nonanginal', 4: 'Nontypical'}, inplace=True)
#dummy for RestECG
restecg_dummy = pd.get_dummies(df.loc[:,'RestECG'])
restecg_dummy.rename(columns={0: 'Normal_restECG', 1: 'Wave_abnormal_restECG',2:'Ventricular_ht_restECG'},inplace=True)
#dummy for Slope
slope_dummy = pd.get_dummies(df['Slope'])
slope_dummy.rename(columns={1: 'Slope_upsloping', 2:'Slope_flat',3: 'Slope_downsloping'},inplace=True)
#dummy for Thal
thal_dummy = pd.get_dummies(df['Thal'])
thal_dummy.rename(columns={3: 'Thal_Normal', 6: 'Thal_fixed',7: 'Thal_reversible'}, inplace=True)
#concatination in data frame
df = pd.concat([df,chestpain_dummy, restecg_dummy, slope_dummy, thal_dummy], axis=1)
#droping Column because their dummies are created
df.drop(['ChestPain','RestECG', 'Slope', 'Thal'], axis=1, inplace=True)

"""##### Checking the No object will left"""

df.info()

"""##### Checking dataset"""

df.head()

"""##### Selecting label means selected Column to predict in df_X and input column in df_y"""

df_X= df.loc[:, df.columns != 'Target']
df_y= df.loc[:, df.columns == 'Target']

"""# Model Training"""

import statsmodels.api as sm
from sklearn.feature_selection import RFE
from sklearn.linear_model import LogisticRegression
selected_features=[]
lr=LogisticRegression()
rfe=RFE(lr,20)

warnings.simplefilter('ignore')
rfe.fit(df_X.values,df_y.values)
print(rfe.support_)
print(rfe.ranking_)

for i, feature in enumerate(df_X.columns.values):
    if rfe.support_[i]:
        selected_features.append(feature)

df_selected_X = df_X[selected_features]
df_selected_y=df_y

lm=sm.Logit(df_selected_y,df_selected_X)
result = lm.fit()

print(result.summary2())
warnings.simplefilter('ignore')

"""##### Spliting Values into test and traning Dataset in the ratio 0.75:0.25"""

from sklearn.model_selection import train_test_split
X_train,X_test, y_train, y_test=train_test_split(df_selected_X,df_selected_y, test_size = 0.25, random_state =0)
columns = X_train.columns

"""##### Calculating Accuracy Function and confusion Matrix of the Models"""

from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score
def cal_accuracy(y_test, y_predict):

    print("\nConfusion Matrix: \n",
    confusion_matrix(y_test, y_predict))

    print (f"\nAccuracy : {accuracy_score(y_test,y_predict)*100:0.3f}")

"""# Logistic Regression"""

lr=LogisticRegression()
lr.fit(X_train,y_train)
print(f"Accuracy of Test Dataset: {lr.score(X_test,y_test):0.3f}")
print(f"Accuracy of Train Dataset: {lr.score(X_train,y_train):0.3f}")
warnings.simplefilter('ignore')

"""##### Vale Prediction for Test dataset for Logistic Regression"""

y_predict=lr.predict(X_test)
print("Predicted values:")
print(y_predict)
cal_accuracy(y_test, y_predict)
print(classification_report(y_test,y_predict))
sns.heatmap(confusion_matrix(y_test,y_predict),annot=True)

"""# Support Vector Machine"""

from sklearn import svm
svm_linear = svm.SVC(kernel='linear')
svm_linear.fit(X_train,y_train)
warnings.simplefilter('ignore')
print(f"Accuracy of Test Dataset: {svm_linear.score(X_test,y_test):0.3f}")
print(f"Accuracy of Train Dataset: {svm_linear.score(X_train,y_train):0.3f}")

"""##### Vale Prediction for Test dataset for SVM"""

y_predict=svm_linear.predict(X_test)
print("Predicted values:")
print(y_predict)
cal_accuracy(y_test, y_predict)
print(classification_report(y_test,y_predict))
sns.heatmap(confusion_matrix(y_test,y_predict),annot=True)

"""# Decision Tree"""

from sklearn.tree import DecisionTreeClassifier
gini = DecisionTreeClassifier(criterion = "gini", random_state =100,max_depth=3, min_samples_leaf=5)
gini.fit(X_train, y_train)
warnings.simplefilter('ignore')
print(f"Accuracy of Test Dataset: {gini.score(X_test,y_test):0.3f}")
print(f"Accuracy of Train Dataset: {gini.score(X_train,y_train):0.3f}")

"""##### Vale Prediction for Test dataset for Decision Tree"""

y_predict=gini.predict(X_test)
print("Predicted values:\n")
print(y_predict)
cal_accuracy(y_test, y_predict)
print(classification_report(y_test,y_predict))
sns.heatmap(confusion_matrix(y_test,y_predict),annot=True)

"""### Desicion Tree Diagram"""

from io import StringIO
from IPython.display import Image
from sklearn.tree import export_graphviz
import pydotplus
dot_data = StringIO()
export_graphviz(decision_tree=gini, out_file=dot_data,
                filled=True, rounded=True,
                special_characters=True, feature_names = X_test.columns,class_names=['0','1'])
graph = pydotplus.graph_from_dot_data(dot_data.getvalue())
graph.write_png('heart-disease-analysis-prediction.png')
Image(graph.create_png())

"""# Random Forest"""

from sklearn.ensemble import RandomForestClassifier

forest=RandomForestClassifier(n_estimators=100)
forest.fit(X_train,y_train)

warnings.simplefilter('ignore')
print(f"Accuracy of Test Dataset: {forest.score(X_test,y_test):0.3f}")
print(f"Accuracy of Train Dataset: {forest.score(X_train,y_train):0.3f}")

"""##### Over Fitting Issue
##### Vale Prediction for Test dataset for Rondom Forest
"""

y_predict=forest.predict(X_test)
print("Predicted values:\n")
print(y_predict)
cal_accuracy(y_test, y_predict)
print(classification_report(y_test,y_predict))
sns.heatmap(confusion_matrix(y_test,y_predict),annot=True)

"""# Naive Bayes"""

from sklearn.naive_bayes import MultinomialNB
nb = MultinomialNB()
nb.fit(X_train,y_train)
warnings.simplefilter('ignore')
print(f"Accuracy of Test Dataset: {nb.score(X_test,y_test):0.3f}")
print(f"Accuracy of Train Dataset: {nb.score(X_train,y_train):0.3f}")

"""##### Vale Prediction for Test dataset for Naive Bayes"""

y_predict = nb.predict(X_test)
print("Predicted values:\n")
print(y_predict)
cal_accuracy(y_test, y_predict)
print(classification_report(y_test,y_predict))
sns.heatmap(confusion_matrix(y_test,y_predict),annot=True)

"""# K Nearest Neighbor(KNN)"""

from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import StandardScaler
sc_X = StandardScaler()
X_train = sc_X.fit_transform(X_train)
X_test = sc_X.transform(X_test)
warnings.simplefilter('ignore')
print(f"Accuracy of Test Dataset: {nb.score(X_test,y_test):0.3f}")
print(f"Accuracy of Train Dataset: {nb.score(X_train,y_train):0.3f}")

classifier = KNeighborsClassifier(n_neighbors = 7, metric = 'minkowski', p = 2)
classifier = classifier.fit(X_train,y_train)

y_predict = classifier.predict(X_test)
print("Predicted values:\n")
print(y_predict)
cal_accuracy(y_test, y_predict)
print(classification_report(y_test,y_predict))
sns.heatmap(confusion_matrix(y_test,y_predict),annot=True)

"""# XGBoost"""

import xgboost as xgb
XGB = xgb.XGBClassifier()
XGB.fit(X_train,y_train)
warnings.simplefilter('ignore')
print(f"Accuracy of Test Dataset: {XGB.score(X_test,y_test):0.3f}")
print(f"Accuracy of Train Dataset: {XGB.score(X_train,y_train):0.3f}")

y_predict=XGB.predict(X_test)
print('Predicted values:\n')
print(y_predict)
cal_accuracy(y_test, y_predict)
print(classification_report(y_test,y_predict))
sns.heatmap(confusion_matrix(y_test,y_predict),annot=True)

"""# AdaBoost"""

from sklearn.ensemble import AdaBoostClassifier
ABC=AdaBoostClassifier()
ABC.fit(X_train,y_train)
warnings.simplefilter('ignore')
print(f"Accuracy of Test Dataset: {ABC.score(X_test,y_test):0.3f}")
print(f"Accuracy of Train Dataset: {ABC.score(X_train,y_train):0.3f}")

y_predict=ABC.predict(X_test)
print('Predicted values:\n')
print(y_predict)
cal_accuracy(y_predict,y_test)
print(classification_report(y_test,y_predict))
sns.heatmap(confusion_matrix(y_test,y_predict),annot=True)

"""# Artificial Neural Network (ANN)"""

from keras.models import Sequential
from keras.layers import Dense

ANN = Sequential()
ANN.add(Dense(11,activation='relu',input_dim=20))
ANN.add(Dense(1,activation='sigmoid'))

ANN.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])

ANN.fit(X_train,y_train,epochs=300)

y_predict=ANN.predict(X_test)
print('Predicted values:\n')
rounded = [round(x[0]) for x in y_predict]
y_predict = rounded
print(y_predict)
cal_accuracy(y_predict,y_test)
print(classification_report(y_test,y_predict))
sns.heatmap(confusion_matrix(y_test,y_predict),annot=True)

"""# Cross Validation For Models"""

from sklearn import model_selection
kfold=model_selection.KFold(n_splits=10, random_state=7)
models=[('Logistic Regression', lr), ('Support Vector Machine', svm_linear), ('Decision Tree', gini),
        ('Random Forest', forest), ('K Nearest Neighbor', classifier),('XGBoost',XGB),('AdaBoostClassifier', ABC)]
warnings.simplefilter('ignore')

for model in models:
    results=model_selection.cross_val_score(model[1],X_train,y_train,cv=kfold,scoring='accuracy')
    print(f"Cross validated Accuracy of  {model[0]}: {results.mean():.3f}")

models=pd.DataFrame({'Model':['Logistics Regression','Support Vector Machine','Decision Tree','Random Forest','Naive Bayes','K Nearest Neighbor','eXtreme Gradient Boosting','AdaBoost'],
                     'Traning Accuracy':[(lr.score(X_train,y_train)),svm_linear.score(X_train,y_train),gini.score(X_train,y_train),forest.score(X_train,y_train),nb.score(X_train,y_train),classifier.score(X_train,y_train),XGB.score(X_train,y_train),ABC.score(X_train,y_train)],
                     'Test Accuracy':[(lr.score(X_test,y_test)),svm_linear.score(X_test,y_test),gini.score(X_test,y_test),forest.score(X_test,y_test),nb.score(X_test,y_test),classifier.score(X_test,y_test),XGB.score(X_test,y_test),ABC.score(X_test,y_test)]})
models.sort_values(by='Test Accuracy', ascending=False)

x=[lr.score(X_test,y_test),svm_linear.score(X_test,y_test),gini.score(X_test,y_test),forest.score(X_test,y_test),nb.score(X_test,y_test),classifier.score(X_test,y_test),XGB.score(X_test,y_test),ABC.score(X_test,y_test)]
y=['Logistics Regression','Support Vector Machine','Decision Tree','Random Forest','Naive Bayes','K Nearest Neighbor','eXtreme Gradient Boosting','AdaBoost']
plt.scatter(x,y)

"""### Best Model for Dataset is Logistic Regression

## Gridsearch on Random Forest to increase the accuracy
"""

n_estimators = [int(x) for x in np.linspace(start = 10, stop = 80, num = 10)]
max_features = ['auto', 'sqrt']
max_depth = [2,4]
min_samples_split = [2, 5]
min_samples_leaf = [1, 2]
param_grid = {'n_estimators': n_estimators,
               'max_features': max_features,
               'max_depth': max_depth,
               'min_samples_split': min_samples_split,
               'min_samples_leaf': min_samples_leaf
             }

from sklearn.ensemble import RandomForestClassifier
model = RandomForestClassifier()

from sklearn.model_selection import GridSearchCV
grid = GridSearchCV(estimator = model, param_grid = param_grid, cv = 3, verbose=2)

grid.fit(X_train, y_train)

grid.best_params_

model = RandomForestClassifier(max_depth=4,max_features='auto',min_samples_leaf=2,min_samples_split=2,n_estimators=25)

model.fit(X_train,y_train)

y_predict = model.predict(X_test)
print(y_pred)
print(confusion_matrix(y_test,y_predict))
print(classification_report(y_test,y_predict))
sns.heatmap(confusion_matrix(y_test,y_predict),annot=True)

"""### Accuacy increased for random forest  """

model.score(X_test,y_test)

"""## Gridsearch on Logistic Regression to increase the accuracy"""

from sklearn.model_selection import GridSearchCV

grid={"C":np.logspace(-3,3,7), "penalty":["l1","l2"]}# l1 lasso l2 ridge
lr_cv=GridSearchCV(lr,grid,cv=10)
lr_cv.fit(X_train, y_train)

lr_cv.best_params_

y_predict=lr_cv.predict(X_test)
print(y_predict)
print(confusion_matrix(y_test,y_predict))
print(classification_report(y_test,y_predict))
sns.heatmap(confusion_matrix(y_test,y_predict),annot=True)

"""### Accuacy increased for Logistic Regression"""

lr_cv.score(X_test, y_test)

"""## Gridsearch on Support Vector Machine to increase the accuracy"""

from sklearn.model_selection import GridSearchCV
from sklearn.svm import SVC
# defining parameter range
param_grid = {'C': [0.1, 1, 10, 100, 1000],
              'gamma': [1, 0.1, 0.01, 0.001, 0.0001],
              'kernel': ['rbf']}

grid = GridSearchCV(SVC(), param_grid, refit = True, verbose = 3)

# fitting the model for grid search
grid.fit(X_train, y_train)

# print best parameter after tuning
print(grid.best_params_)

# print how our model looks after hyper-parameter tuning
print(grid.best_estimator_)

y_predict=grid.predict(X_test)
print("Predicted values:")
print(y_predict)
print("\nConfusion Matrix: \n",confusion_matrix(y_test, y_predict))
print("\nClassification Report: \n",classification_report(y_test,y_predict))
sns.heatmap(confusion_matrix(y_test,y_predict),annot=True)

"""### Accuacy increased for Support Vector Machine"""

grid.score(X_test,y_test)

"""## Randomized Search on eXtreme Gradient Boosting to increase the accuracy"""

xgb_classifier = xgb.XGBClassifier()

gbm_param_grid = {
    'n_estimators': range(1,20),
    'max_depth': range(1, 10),
    'learning_rate': [.1,.4, .45, .5, .55, .6],
    'colsample_bytree': [.6, .7, .8, .9, 1],
    'booster':["gbtree"],
     'min_child_weight': [0.001,0.003,0.01],
}

from sklearn.model_selection import RandomizedSearchCV
xgb_random = RandomizedSearchCV(param_distributions=gbm_param_grid,
                                    estimator = xgb_classifier, scoring = "accuracy",
                                    verbose = 0, n_iter = 100, cv = 4)

xgb_random.fit(X_train, y_train)

xgb_bp = xgb_random.best_params_

xgb_model=xgb.XGBClassifier(n_estimators=xgb_bp["n_estimators"],
                            min_child_weight=xgb_bp["min_child_weight"],
                            max_depth=xgb_bp["max_depth"],
                            learning_rate=xgb_bp["learning_rate"],
                            colsample_bytree=xgb_bp["colsample_bytree"],
                            booster=xgb_bp["booster"])

xgb_model.fit(X_train, y_train)

y_predict=xgb_model.predict(X_test)
print("Predicted values:")
print(y_predict)
print("\nConfusion Matrix: \n",confusion_matrix(y_test, y_predict))
print("\nClassification Report: \n",classification_report(y_test,y_predict))
sns.heatmap(confusion_matrix(y_test,y_predict),annot=True)

"""### Accuacy increased for  XGBoost (eXtreme Gradient Boosting)"""

xgb_model.score(X_test, y_test)

"""## Gridsearch on AdaBoost to increase accuracy"""

shallow_tree = DecisionTreeClassifier(max_depth=1, random_state = 100)

from sklearn import metrics
estimators = list(range(20,25))

abc_scores = []
for n_est in estimators:
    ABC = AdaBoostClassifier(
    base_estimator=shallow_tree,
    n_estimators = n_est)

    ABC.fit(X_train, y_train)
    y_pred = ABC.predict(X_test)
    score = metrics.accuracy_score(y_test, y_pred)
    abc_scores.append(score)

# plot test scores and n_estimators
# plot
plt.plot(estimators, abc_scores)
plt.xlabel('n_estimators')
plt.ylabel('accuracy')
plt.ylim([0.70, 1])
plt.show()

ABC = AdaBoostClassifier(
    base_estimator=shallow_tree,
    n_estimators = 21)

ABC.fit(X_train, y_train)
y_predict = ABC.predict(X_test)

print("Predicted values:")
print(y_predict)
print("\nConfusion Matrix: \n",confusion_matrix(y_test, y_predict))
print("\nClassification Report: \n",classification_report(y_test,y_predict))
sns.heatmap(confusion_matrix(y_test,y_predict),annot=True)

"""### Accuacy increased for  AdaBoost"""

print(accuracy_score(y_test,y_predict))

"""## Gridsearch on Neural Network to increase accuracy"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense,Dropout
from tensorflow.keras.callbacks import EarlyStopping

from keras import regularizers
from keras.optimizers import Adam
def create_binary_model():
    # create model
    model = Sequential()
    model.add(Dense(16, input_dim=20, kernel_initializer='normal',  kernel_regularizer=regularizers.l2(0.001),activation='relu'))
    model.add(Dropout(0.25))
    model.add(Dense(8, kernel_initializer='normal',  kernel_regularizer=regularizers.l2(0.001),activation='relu'))
    model.add(Dropout(0.25))
    model.add(Dense(1, activation='sigmoid'))

    # Compile model
    adam = Adam(lr=0.001)
    model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])
    return model

binary_model = create_binary_model()

print(binary_model.summary())

history=binary_model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, batch_size=10)

y_predict=binary_model.predict(X_test)
print('Predicted values:\n')
rounded = [round(x[0]) for x in y_predict]
y_predict = rounded
print(y_predict)
cal_accuracy(y_predict,y_test)
print(classification_report(y_test,y_predict))
sns.heatmap(confusion_matrix(y_test,y_predict),annot=True)

"""### Accuacy increased for Neural Network"""

print(accuracy_score(y_test,y_predict))

"""## Gridsearch on k-Nearest Neighbours to increase accuracy"""

KNN = KNeighborsClassifier()
from sklearn.model_selection import GridSearchCV
params_KNN = dict(n_neighbors = range(1,10))
grid_search_KNN = GridSearchCV(KNN, param_grid = params_KNN, cv =4, scoring='recall')
grid_search_KNN.fit(X_train,y_train)

KNN_best_k = grid_search_KNN.best_params_['n_neighbors']
print("For a k-Nearest Neighbors model, the optimal value of k is "+str(KNN_best_k))
KNN_df = pd.DataFrame(grid_search_KNN.cv_results_)
fig_KNN = plt.figure(figsize=(12,9))
plt.plot(KNN_df['param_n_neighbors'],KNN_df['mean_test_score'],'b-o')
plt.xlim(0,10)
plt.ylim(0.5,1.0)
plt.xlabel('k')
plt.ylabel('Mean recall over 4 cross-validation sets')

classifier = KNeighborsClassifier(n_neighbors = 9, metric = 'minkowski', p = 2)
classifier = classifier.fit(X_train,y_train)

y_predict = classifier.predict(X_test)
print("Predicted values:\n")
print(y_predict)
cal_accuracy(y_test, y_predict)
print(classification_report(y_test,y_predict))
sns.heatmap(confusion_matrix(y_test,y_predict),annot=True)

print(accuracy_score(y_test,y_predict))

models=pd.DataFrame({'Model':['Random Forest','Logistics Regression','eXtreme Gradient Boosting','AdaBoost','SVM'],
                     'Traning Accuracy':[(model.score(X_train,y_train)),lr_cv.score(X_train,y_train),xgb_model.score(X_train,y_train),ABC.score(X_train,y_train),grid.score(X_train,y_train)],
                     'Test Accuracy':[(model.score(X_test,y_test)),lr_cv.score(X_test,y_test),xgb_model.score(X_test,y_test),ABC.score(X_test,y_test),grid.score(X_test,y_test)]})
models.sort_values(by='Test Accuracy', ascending=False)

"""# For manulally inputting data and finding whether Heart Disease or not"""

print("Enter Patients Name:")
name = input()
print("Enter Patients Age:")
Age = int(input())
print("Enter Patients Gender (if male=1,if female=0):")
Sex = int(input())
print("Is your Fasting Blood Sugar > 120 mg/dl? (1=True, 0=False)")
Fbs = int(input())
print("What is your maximum heart rate achieved")
MaxHR = int(input())
print("Do you have Exercise Induced Angina? (1=True, 0=False)")
ExAng = int(input())
print("What is your ST depression induced by exercise relative to rest?")
Oldpeak = float(input())
print("What is your number of major vessels (0-4) colored by flourosopy ")
Ca = int(input())
print("Do you have Asymptomatic Chest pain? (1=True, 0=False)")
ACP = int(input())
print("Do you have Non-anginal Chest pain? (1=True, 0=False)")
NACP = int(input())
print("Do you have Non-typical Chest pain? (1=True, 0=False)")
NTCP = int(input())
print("Do you have typical Chest pain? (1=True, 0=False)")
TCP = int(input())
print("Do you have normal Resting Electrocardiographic? (1=True, 0=False)")
NREC = int(input())
print("Do you have Wave abnormal Resting Electrocardiographic? (1=True, 0=False)")
WAREC = int(input())
print("Do you have Ventricular Resting Electrocardiographic? (1=True, 0=False)")
VREC = int(input())
print("Do you have slope Upsloping? (1=True, 0=False)")
SUPS = int(input())
print("Do you have slope flat? (1=True, 0=False)")
Sf = int(input())
print("Do you have slope Downsloping? (1=True, 0=False)")
SDS = int(input())
print("Do you have fixed Thal? (1=True, 0=False)")
FT = int(input())
print("Do you have Normal Thal? (1=True, 0=False)")
NT = int(input())
print("Do you have Reversable Thal? (1=True, 0=False)")
RT = int(input())

arr = [[Age,Sex,Fbs,MaxHR,ExAng,Oldpeak,Ca,ACP,NACP,NTCP,TCP,NREC,WAREC,VREC,SUPS,Sf,SDS,FT,NT,RT]]
x = model.predict(arr)[0]
[]
perc = str(int(model.predict_proba(arr)[0,1]*100))
print('Hello ' + name + '!')
if (x==0):
    print('You are safe from any kind of Heart Diseases')
    print('You have only '+perc+'% chances of getting a Heart Disease which is normal for a healthy human being')
else:
    print('You have a Heart Disease')
    print('You have a high '+perc+'% chances of getting a Heart Disease. You must immediately consult a doctor')

print("Enter Patients Name:")
name = input()
print("Enter Patients Age:")
Age = int(input())
print("Enter Patients Gender (if male=1,if female=0):")
Sex = int(input())
print("Is your Fasting Blood Sugar > 120 mg/dl? (1=True, 0=False)")
Fbs = int(input())
print("What is your maximum heart rate achieved")
MaxHR = int(input())
print("Do you have Exercise Induced Angina? (1=True, 0=False)")
ExAng = int(input())
print("What is your ST depression induced by exercise relative to rest?")
Oldpeak = float(input())
print("What is your number of major vessels (0-4) colored by flourosopy ")
Ca = int(input())
print("Do you have Asymptomatic Chest pain? (1=True, 0=False)")
ACP = int(input())
print("Do you have Non-anginal Chest pain? (1=True, 0=False)")
NACP = int(input())
print("Do you have Non-typical Chest pain? (1=True, 0=False)")
NTCP = int(input())
print("Do you have typical Chest pain? (1=True, 0=False)")
TCP = int(input())
print("Do you have normal Resting Electrocardiographic? (1=True, 0=False)")
NREC = int(input())
print("Do you have Wave abnormal Resting Electrocardiographic? (1=True, 0=False)")
WAREC = int(input())
print("Do you have Ventricular Resting Electrocardiographic? (1=True, 0=False)")
VREC = int(input())
print("Do you have slope Upsloping? (1=True, 0=False)")
SUPS = int(input())
print("Do you have slope flat? (1=True, 0=False)")
Sf = int(input())
print("Do you have slope Downsloping? (1=True, 0=False)")
SDS = int(input())
print("Do you have fixed Thal? (1=True, 0=False)")
FT = int(input())
print("Do you have Normal Thal? (1=True, 0=False)")
NT = int(input())
print("Do you have Reversable Thal? (1=True, 0=False)")
RT = int(input())

arr = [[Age,Sex,Fbs,MaxHR,ExAng,Oldpeak,Ca,ACP,NACP,NTCP,TCP,NREC,WAREC,VREC,SUPS,Sf,SDS,FT,NT,RT]]
x = model.predict(arr)[0]
[]
perc = str(int(model.predict_proba(arr)[0,1]*100))
print('Hello ' + name + '!')
if (x==0):
    print('You are safe from any kind of Heart Diseases')
    print('You have only '+perc+'% chances of getting a Heart Disease which is normal for a healthy human being')
else:
    print('You have a Heart Disease')
    print('You have a high '+perc+'% chances of getting a Heart Disease. You must immediately consult a doctor')

"""# Feature Importance

### Logistic regression
"""

# Fit the instance of LogisticRegression
clf = LogisticRegression(C=0.38566204211634725,
                        solver="liblinear")
clf.fit(X_train, y_train);

# Checking coefficients
clf.coef_

# Match coef's of features to columns

feature_dictionary_lr = dict(zip(df.columns, list(clf.coef_[0])))
feature_dictionary_lr

# Visualize the feature importance
feature_df = pd.DataFrame(feature_dictionary_lr, index=[0])
feature_df.T.plot.bar(title="Feature Importance", legend=False);

"""### Decision Tree"""

gini.feature_importances_

feature_dictionary_dt = dict(zip(df.columns, list(gini.feature_importances_)))
feature_dictionary_dt

# Visualize the feature importance
feature_df = pd.DataFrame(feature_dictionary_dt, index=[0])
feature_df.T.plot.bar(title="Feature Importance", legend=False);

"""### Random Forest"""

forest.feature_importances_

feature_dictionary_rf = dict(zip(df.columns, list(forest.feature_importances_)))
feature_dictionary_rf

# Visualize the feature importance
feature_df = pd.DataFrame(feature_dictionary_rf, index=[0])
feature_df.T.plot.bar(title="Feature Importance", legend=False);

"""### XGBoost (eXtreme Gradient Boosting)"""

XGB.feature_importances_

feature_dictionary_XGB = dict(zip(df.columns, list(XGB.feature_importances_)))
feature_dictionary_XGB

# Visualize the feature importance
feature_XGB = pd.DataFrame(feature_dictionary_XGB, index=[0])
feature_XGB.T.plot.bar(title="Feature Importance", legend=False);

"""### AdaBoost"""

ABC.feature_importances_

feature_dictionary_ABC = dict(zip(df.columns, list(ABC.feature_importances_)))
feature_dictionary_ABC

# Visualize the feature importance
feature_XGB = pd.DataFrame(feature_dictionary_XGB, index=[0])
feature_XGB.T.plot.bar(title="Feature Importance", legend=False);